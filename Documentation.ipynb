{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Documentation.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Pzjep1_pMbM0","colab_type":"text"},"source":["# CS-GY 9223-D: Deep Learning Homework 2\n","\n","## Documentation\n","\n","Member 1: Harshvardhan Dudeja, hd1090\n","\n","Member 2: Videet Parekh, vrp288\n","\n","Member 3: Ankur Bhatkalkar, ab7869 "]},{"cell_type":"markdown","metadata":{"id":"3mHS77koMbM1","colab_type":"text"},"source":["â€“ Duration of training: approx 3 hours each for both ResNet50 and Xception (30 epochs)"]},{"cell_type":"markdown","metadata":{"id":"SZHW0zSdMbM3","colab_type":"text"},"source":["### Final Project - What it was\n","\n","- For our Final Project (ECE-GY 6143 : Machine Learning), our team had first decided to build a classifier for Men's Fashion(classifies a combination of clothes as good or bad).\n","- While working on the project, we assumed that the data was cleaned in the 'fashion' we wanted it in.\n","- To solve the problem we explored a number of options like Ensemble Learning, Collaborative Learning and Tranfer Learning on pretrained models like  VGG16, DenseNet, Resnet50 and Xception.\n","\n","- While Collaborative Learning seems the best fit for the fashion problem (Since most of the fashion sense is derived from what other people think of it), learning and implementing the ALS method in Spark or other sklearn models along with cleaning and manually labbeling almost 1 million images was beyond the time frame we had. \n","\n","*We did try and learn a lot from it though.*\n","\n","### Final Project - Improvizing\n","\n","- Since we had done so much research we thought of applying these techniques to a problem which had the data ready and applying these Deep Learning Techniques to it to check if we can do better than the exisitng solutions.\n","\n","- We choose the GalaxyZoo classification challenge on Kaggle and applied Ensemble Learning with ResNet50 and Xception (along with fine tuning the pre-trained layers) and the results were very promising. \n","\n","\n","### VGG16: \n","\n","- Started with VGG16 base kernel. Made changes in the dataset to get validation dataset. Achieved Score - 0.12501\n","- Implemented offline data augmentation (200,000 images), and trained using VGG16 model. Achieved Score - 0.11754\n","\n","### Resnet50:\n","\n","- Resnet50 without training initial layers gave accuracy of 0.09780. \n","- Training all the layers gave boost till 0.06251. \n","- Added Keras callback in the model to give best weights for each 3rd iteration. Score - 0.05282\n","- Tried improving the model with different optimizers. List of Optimizers experimented with - SGD, adam, nadam, RMSE, AdaBound, AdamW. All of these either produced lower or similar validation accuracy.\n","\n","### Xception:\n","\n","- Fine tuning of all layers, adam with decay, no zooming in images, online data augmentation - mainly rotation with NVIDIA V100 GPU, 16 CPUs. Boost till 0.03697. \n","- Further tried to improve the model with different optimizers, changes in data augmentation methods (zooming, ZCA whitening etc), changing no. of epochs. Couldn't beat the previous score. Score achieved in the range of 0.04234 - 0.04533\n","- Since adam showed the most promising results - we trained our Xception model for 30 epochs. \n","\n","### Ensembling (Resnet50 & Xception):\n","\n","- Ensemble of two Xception models showed significant improvement in the result. \n","- Seeing the improvement we decided to go with Resnet50 + Xception. Both trained for 30 epochs with similar configurations. Achieved score of 0.03503.\n","\n","### Observations and freebies left unsused due to time limit:\n","\n","- Optimizers and how you implement them according to problem statement plays an important role. AdaBound and AdamW both looked promising and something we would keep in our pockets for future use. \n","- Experimentation with Data Augmentation, batch size and image size - not every feature in ImageDataGenerator is useful. \n","- Ensembles we wanted to try - Densnet + VGG16 + Resnet50 + Xception "]}]}